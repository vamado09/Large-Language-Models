{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycWrCwa0kMtv"
      },
      "source": [
        "# Hugging Face + LangChain: Text Summerization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v_SGJ1ildQF"
      },
      "source": [
        "## Goal:\n",
        "\n",
        "The primary goal is to create a tool that can read text from different file formats (like Markdown, PDF, and DOCX) and produce a concise, informative summary of the content. This is useful for quickly understanding the main points of long documents without having to read the entire content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QouVj8AfoGs"
      },
      "outputs": [],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-mvZzz7fo3p"
      },
      "outputs": [],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_-hkGLCeutC",
        "outputId": "5d588b51-cab0-4d16-b6d7-e537f95fb665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from docx import Document\n",
        "import fitz  # PyMuPDF\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def read_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    return \" \".join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "def read_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with fitz.open(file_path) as doc:\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def read_markdown(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def format_summary(summary):\n",
        "    sentences = sent_tokenize(summary)\n",
        "    formatted_summary = \"\\n\".join(sentences)\n",
        "    return formatted_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGh0P5PkbrX-"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai tiktoken transformers accelerate cohere --quiet # install libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTXk9RwlnLZm"
      },
      "source": [
        "**Libraries explained**:\n",
        "\n",
        "- **langchain**: This is a Python library designed to facilitate working with large language models (LLMs). It provides tools and functionalities to make it easier to interact with models like those provided by OpenAI (GPT-3, for example), making it useful for tasks like text summarization, question-answering, and more.\n",
        "\n",
        "- **openai**: This is the official Python library provided by OpenAI. It's used to interact with the OpenAI API, which provides access to models like GPT-3.5 or 4. This library is essential if we're planning to integrate OpenAI's powerful language models into our application.\n",
        "\n",
        "- **tiktoken**: This library is less commonly known and might be used for specific tokenization tasks or for working with certain types of text data. Tokenization is a fundamental step in NLP where text is broken down into smaller units like words or phrases.\n",
        "\n",
        "- **transformers**: Developed by Hugging Face, the transformers library offers a vast collection of pre-trained models like BERT, GPT, T5, etc., for various NLP tasks. It's a highly versatile library, enabling tasks like text classification, summarization, translation, and more.\n",
        "\n",
        "- **accelerate**: Also from Hugging Face, accelerate is used to simplify and accelerate training and inference processes for deep learning models. It helps in easily running models on different hardware (CPUs, GPUs) with minimal changes in the code.\n",
        "\n",
        "- **cohere**: Cohere is another AI platform similar to OpenAI, providing large language models for various tasks. The cohere library is their Python SDK, allowing easy interaction with Cohere's models for NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P--oMS4HcBEQ"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFaceHub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdKNQ_x9lj68"
      },
      "source": [
        "Using pre-trained model from the Hugging Face model repository. Specifically, I'm using the **\"facebook/bart-large-cnn\"** model.\n",
        "\n",
        "\n",
        "- **BART (Bidirectional and Auto-Regressive Transformers)**: It's a powerful and versatile NLP model capable of handling various tasks, including text summarization. The model is based on the Transformer architecture, which has significantly advanced the field of natural language processing.\n",
        "\n",
        "- **Large-CNN Variant**: The '**large-cnn**' variant of BART is particularly fine-tuned for summarization tasks. It's optimized to generate summaries that are both coherent and closely aligned with the main points of the input text.\n",
        "\n",
        "This should help us use the BART large CNN model from Hugging Face for summarization without encountering the validation error.\n",
        "\n",
        "Source: https://huggingface.co/blog/Andyrasika/agent-helper-langchain-hf\n",
        "\n",
        "Source: https://huggingface.co/docs/hub/en/security-tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GRP8EVkcCgK",
        "outputId": "5ca27ebf-fa1e-4487-fd01-bbb437137df7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "huggingface_api_token = 'your_hugginface_token'# you need to create a HuggingFace account and generate your token\n",
        "\n",
        "# HuggingFaceHub:summarizer object\n",
        "# repo_id: repository where bart-large-cnn is\n",
        "# facebook/bart-large-cnn: model being used\n",
        "# model_kwargs: keyword arguments passed to the mdel  -> temperature and max_length\n",
        "\n",
        "summarizer = HuggingFaceHub(repo_id=\"facebook/bart-large-cnn\", # powerful model for summarization task\n",
        "                            model_kwargs={\"temperature\": 0, \"max_length\": 180}, # temp 0 means no randonmness, length 180 characters\n",
        "                            huggingfacehub_api_token=huggingface_api_token) # token\n",
        "\n",
        "def summarize(llm, text) -> str:\n",
        "    return llm.invoke(f\"Summarize this: {text}!\")\n",
        "\n",
        "#def summarize(llm, text) -> str:\n",
        "    #return llm(f\"Summarize this: {text}!\").summarize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "feGoGHlqfBR0"
      },
      "outputs": [],
      "source": [
        "file_path = ''  # file path -> /content/Data_Curation_KO.docx\n",
        "\n",
        "if file_path.endswith('.docx'): # word document\n",
        "    text = read_docx(file_path)\n",
        "elif file_path.endswith('.pdf'): # pdf file\n",
        "    text = read_pdf(file_path)\n",
        "elif file_path.endswith('.md'): # markdown file\n",
        "    text = read_markdown(file_path)\n",
        "else:\n",
        "    text =  '''\n",
        "    As mentioned before, this tool is built entirely in JavaScript.\n",
        "    Although interpreted languages like JavaScript have classically been deemed too inefficient for running simulations, the creators found that this no longer holds: investments by major tech companies have tremendously improved JavaScript engines over the past years, to the point that our CPM now has no major performance disadvantage compared to existing C++ frameworks.\n",
        "    The JavaScript implementation of Artistoo opens new possibilities for rapid and low barrier sharing of CPM simulations with students, collaborators, and readers or reviewers of a paper.\n",
        "    Unlike existing frameworks, Artistoo allows building simulations that run in the web browser without the need to install any software. Artistoo models run on any platform providing a standards-compliant web browser â€“ be it a desktop computer, a tablet, or a mobile phone. These simulations can be published on any web server or saved locally and do not rely on any back-end servers being available. They can be made explorable, enabling viewers to interact with the simulation and see the effect of changing model parameters in real time.\n",
        "    Artistoo is a JavaScript library implemented as an ECMAScript 6 module, which can be loaded into an HTML page or accessed from within a Node.js command line application.\n",
        "    '''\n",
        " # insert text if not docx, pdf, or markdwon file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example use: using random text from \"Artistoo\" knowledge object"
      ],
      "metadata": {
        "id": "iUwOZ0qIUGEw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP1fYcm5maQs"
      },
      "source": [
        "## Results:\n",
        "\n",
        "Observation: maybe I can increase the \"max_length\" parameter to generate longer results. However, long \"max_length\" might lead to redundant or less coherent summeries ( I don't know yet*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABfF4vWvf7ky",
        "outputId": "491ed0c5-7d98-4954-e778-dae5b9532846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artistoo allows building simulations that run in the web browser without the need to install any software.\n",
            "Artistoo models run on any platform providing a standards-compliant web browser â€“ be it a desktop computer, a tablet, or a mobile phone.\n",
            "The JavaScript implementation of Artistoo opens new possibilities for rapid and low barrier sharing of CPM simulations.\n"
          ]
        }
      ],
      "source": [
        "summary = summarize(summarizer, text)\n",
        "formatted_summary = format_summary(summary) # formatting results\n",
        "\n",
        "print(formatted_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_lHy0Gjy--H"
      },
      "source": [
        "## RAG + OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPcaUn8EdmeR"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf -q -U\n",
        "# To bbe continue.."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}